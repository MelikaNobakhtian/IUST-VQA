{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torch","metadata":{"execution":{"iopub.status.busy":"2021-10-06T20:01:16.93543Z","iopub.execute_input":"2021-10-06T20:01:16.935747Z","iopub.status.idle":"2021-10-06T20:01:46.458914Z","shell.execute_reply.started":"2021-10-06T20:01:16.935713Z","shell.execute_reply":"2021-10-06T20:01:46.457356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport json\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gensim\nimport gensim.downloader as api\nimport torchtext\nimport torch\nfrom torch import nn","metadata":{"id":"pttt51N1GTCd","execution":{"iopub.status.busy":"2021-10-06T20:03:13.621518Z","iopub.execute_input":"2021-10-06T20:03:13.622409Z","iopub.status.idle":"2021-10-06T20:03:16.32846Z","shell.execute_reply.started":"2021-10-06T20:03:13.622354Z","shell.execute_reply":"2021-10-06T20:03:16.327076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/minivqaiust/'","metadata":{"id":"hpd1b2MRJv3b","execution":{"iopub.status.busy":"2021-10-06T20:03:21.132018Z","iopub.execute_input":"2021-10-06T20:03:21.132968Z","iopub.status.idle":"2021-10-06T20:03:21.137271Z","shell.execute_reply.started":"2021-10-06T20:03:21.132926Z","shell.execute_reply":"2021-10-06T20:03:21.136252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(path + \"image_features.pickle\", 'rb') as f:\n    img = pickle.load(f)\nwith open(path + \"image_question.json\") as json_file:\n      img_q = json.load(json_file)","metadata":{"id":"6AlAPs_UMGLD","execution":{"iopub.status.busy":"2021-10-06T20:05:30.567057Z","iopub.execute_input":"2021-10-06T20:05:30.567407Z","iopub.status.idle":"2021-10-06T20:05:30.582093Z","shell.execute_reply.started":"2021-10-06T20:05:30.567376Z","shell.execute_reply":"2021-10-06T20:05:30.581058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path + \"train.csv\")\nq_train_idx = list(df['question_id'])\nlabel_train = list(df['label'])\ndf = pd.read_csv(path + \"val.csv\")\nq_val_idx = list(df['question_id'])\nlabel_val = list(df['label'])\ndf = pd.read_csv(path + \"test.csv\")\nq_test_idx = list(df['question_id'])","metadata":{"id":"TPLRojcmM_uP","execution":{"iopub.status.busy":"2021-10-06T20:05:35.019682Z","iopub.execute_input":"2021-10-06T20:05:35.019991Z","iopub.status.idle":"2021-10-06T20:05:35.078556Z","shell.execute_reply.started":"2021-10-06T20:05:35.019961Z","shell.execute_reply":"2021-10-06T20:05:35.07745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions_train = []\nimage_features_train = []\nall_qs = {}\n\n#change format for better performing\nfor idx, imq in img_q.items():\n  for ques in imq:\n    all_qs[ques[0]] = {'question':ques[1], 'image_id': str(idx)}\nall_qs[131087000]","metadata":{"id":"12hGoIH1xxBz","outputId":"ca3601b5-14f6-4ada-e4d3-f7f695253909","execution":{"iopub.status.busy":"2021-10-06T20:05:40.593847Z","iopub.execute_input":"2021-10-06T20:05:40.595093Z","iopub.status.idle":"2021-10-06T20:05:40.610388Z","shell.execute_reply.started":"2021-10-06T20:05:40.595038Z","shell.execute_reply":"2021-10-06T20:05:40.609338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in q_train_idx:\n  questions_train.append(all_qs[idx]['question'])\n  image_features_train.append(img[all_qs[idx]['image_id']])","metadata":{"id":"QaPoGa4zz9Zw","execution":{"iopub.status.busy":"2021-10-06T20:05:46.373894Z","iopub.execute_input":"2021-10-06T20:05:46.374797Z","iopub.status.idle":"2021-10-06T20:05:46.38255Z","shell.execute_reply.started":"2021-10-06T20:05:46.374731Z","shell.execute_reply":"2021-10-06T20:05:46.381361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions_val = []\nimage_features_val = []\n\nfor idx in q_val_idx:\n  questions_val.append(all_qs[idx]['question'])\n  image_features_val.append(img[all_qs[idx]['image_id']])","metadata":{"id":"0W9s_nP11zZT","execution":{"iopub.status.busy":"2021-10-06T20:05:50.474863Z","iopub.execute_input":"2021-10-06T20:05:50.47516Z","iopub.status.idle":"2021-10-06T20:05:50.480844Z","shell.execute_reply.started":"2021-10-06T20:05:50.475132Z","shell.execute_reply":"2021-10-06T20:05:50.479797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions_test = []\nimage_features_test = []\n\nfor idx in q_test_idx:\n  questions_test.append(all_qs[idx]['question'])\n  image_features_test.append(img[all_qs[idx]['image_id']])","metadata":{"id":"OnJzbc-f2Gwr","execution":{"iopub.status.busy":"2021-10-06T20:05:53.968524Z","iopub.execute_input":"2021-10-06T20:05:53.968828Z","iopub.status.idle":"2021-10-06T20:05:53.974968Z","shell.execute_reply.started":"2021-10-06T20:05:53.968799Z","shell.execute_reply":"2021-10-06T20:05:53.973615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextEmbedding(nn.Module):\n    def __init__(self, vocab_size, embed_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n\n    def forward(self, q):\n        x = self.embedding(q)\n        return x","metadata":{"id":"5Cg1nbeAjdzk","execution":{"iopub.status.busy":"2021-10-06T20:08:35.590351Z","iopub.execute_input":"2021-10-06T20:08:35.590902Z","iopub.status.idle":"2021-10-06T20:08:35.597897Z","shell.execute_reply.started":"2021-10-06T20:08:35.590864Z","shell.execute_reply":"2021-10-06T20:08:35.597109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_model = api.load('word2vec-google-news-300')","metadata":{"id":"I_MIjxz2cmbw","outputId":"bf112cd8-afba-41e2-c5f2-fd5985cc548f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = torchtext.data.utils.get_tokenizer('basic_english')","metadata":{"id":"vNJgqdfvdqv-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = list(pre_model.vocab.keys())\nembed_size = len(pre_model.get_vector('me'))","metadata":{"id":"529Fbu1NlmTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_text = TextEmbedding(\n    vocab_size = len(vocab) + 1, \n    embed_dim = embed_size\n)","metadata":{"id":"pbYeivNzlexb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_dict = {}\nfor idx , word in enumerate(vocab):\n  word_dict[word] = idx\nword_dict['me']","metadata":{"id":"g6lSfg7Bd095","outputId":"dbbd98aa-dbc2-4b3d-f823-1187b9d4d578"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode(seq):\n  code = []\n  for tok in tokenizer(seq):\n    try:\n      code.append(word_dict[tok])\n    except:\n      code.append(len(vocab))\n  return code","metadata":{"id":"GQE26mDh33Hz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def padify(b):\n  v = [encode(x) for x in b]\n  l = max(map(len,v))\n  return torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])","metadata":{"id":"ngTi-FDe3ADd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_question_pad = padify(questions_train)\nval_question_pad = padify(questions_val)","metadata":{"id":"2KwfI96C6Nm4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_question_pad = padify(questions_test)","metadata":{"id":"g0sIhURMBrlL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n  question_features_train = process_text(train_question_pad)\nwith torch.no_grad():\n  question_features_val = process_text(val_question_pad)","metadata":{"id":"zS1l3Fco6vPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n  question_features_test = process_text(test_question_pad)","metadata":{"id":"i0qN0T8xBvGm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = torch.utils.data.TensorDataset(question_features_train, torch.tensor(image_features_train), torch.tensor(label_train))\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{"id":"SAS1stUvGkGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataset = torch.utils.data.TensorDataset(question_features_val, torch.tensor(image_features_val), torch.tensor(label_val))\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)","metadata":{"id":"cQUcmdAjHauN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VQA(nn.Module):\n    def __init__(self, features_size):\n        super(type(self), self).__init__()\n        self.lstm = nn.LSTM(300, 512, num_layers=2, dropout=0.1, batch_first=True)\n        self.linear = nn.Sequential(\n            nn.BatchNorm1d(features_size),\n            nn.Linear(features_size, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),         \n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Linear(128, 10),\n            nn.BatchNorm1d(10),\n        )\n      \n      \n            \n        \n    def forward(self, text, image):\n        text_feature = self.lstm(text)[0]\n        text_feature = torch.mean(text_feature, 1)\n        x = torch.cat([text_feature, image], dim=1)\n        logits = self.linear(x)\n        logits = nn.functional.softmax(logits, dim=1)\n        return logits","metadata":{"id":"kzt-GjSEHv5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model = VQA(1024)","metadata":{"id":"V2Uyxj0eLlsx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.3\nepochs = 30\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(final_model.parameters(), lr=learning_rate)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)","metadata":{"id":"B8dJxP5YN7ix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (text, image, y) in enumerate(dataloader):        \n        # Compute prediction and loss\n        pred = model(text, image)\n        #print('hello')\n        loss = loss_fn(pred, y)\n        \n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        #print(batch)\n\n        if batch % 2 == 0:\n            loss, current = loss.item(), batch * len(text)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for text, image, y in dataloader:\n            pred = model(text, image)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n            \n    test_loss /= size\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{"id":"xuxb7BVUPO0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, final_model, loss_fn, optimizer)\n    test_loop(val_dataloader, final_model, loss_fn)\n    scheduler.step()\nprint(\"Done!\")","metadata":{"id":"qNWw6GLMQSPR","outputId":"8d1780cc-76c1-4b45-d5a1-3f155abe6327"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = final_model(question_features_test, torch.tensor(image_features_test))","metadata":{"id":"0vIY9_0tYAAP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = [int(out.argmax(0).numpy()) for out in y]\nlabeldict = {}\nlabeldict['question_id'] = q_test_idx\nlabeldict['label'] = []\nfor idx, out in enumerate(results):\n  labeldict['label'].append(int(out))","metadata":{"id":"Ja21mWHtCbBO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfl = pd.DataFrame(labeldict)","metadata":{"id":"NRg8nP7zKGVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfl","metadata":{"id":"HigqT_BhnsPb","outputId":"593af9d9-1f13-452e-81be-3d24077d42d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfl.to_csv(path + 'testvqa1_last.csv', index=False)","metadata":{"id":"GJQN7UYkK2Hq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n   ","metadata":{"id":"kq4QJiI0OHxb"},"execution_count":null,"outputs":[]}]}