{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:52:38.778404Z","iopub.execute_input":"2021-10-08T07:52:38.778747Z","iopub.status.idle":"2021-10-08T07:53:07.026171Z","shell.execute_reply.started":"2021-10-08T07:52:38.778716Z","shell.execute_reply":"2021-10-08T07:53:07.025085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport transformers\nimport json\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gensim\nimport gensim.downloader as api\nimport torchtext\nimport torch\nfrom torch import nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-08T07:53:11.627781Z","iopub.execute_input":"2021-10-08T07:53:11.628352Z","iopub.status.idle":"2021-10-08T07:53:11.828844Z","shell.execute_reply.started":"2021-10-08T07:53:11.628303Z","shell.execute_reply":"2021-10-08T07:53:11.827586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/minivqaiust/'","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:51:52.145562Z","iopub.execute_input":"2021-10-08T07:51:52.145876Z","iopub.status.idle":"2021-10-08T07:51:52.150525Z","shell.execute_reply.started":"2021-10-08T07:51:52.145848Z","shell.execute_reply":"2021-10-08T07:51:52.149629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(path + \"image_features.pickle\", 'rb') as f:\n    img = pickle.load(f)\nwith open(path + \"image_question.json\") as json_file:\n      img_q = json.load(json_file)","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:51:56.3891Z","iopub.execute_input":"2021-10-08T07:51:56.389814Z","iopub.status.idle":"2021-10-08T07:51:56.509029Z","shell.execute_reply.started":"2021-10-08T07:51:56.38976Z","shell.execute_reply":"2021-10-08T07:51:56.507811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(path + \"train.csv\")\nq_train_idx = list(df['question_id'])\nlabel_train = list(df['label'])\ndf = pd.read_csv(path + \"val.csv\")\nq_val_idx = list(df['question_id'])\nlabel_val = list(df['label'])\ndf = pd.read_csv(path + \"test.csv\")\nq_test_idx = list(df['question_id'])","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:52:03.908016Z","iopub.execute_input":"2021-10-08T07:52:03.908716Z","iopub.status.idle":"2021-10-08T07:52:03.924053Z","shell.execute_reply.started":"2021-10-08T07:52:03.908672Z","shell.execute_reply":"2021-10-08T07:52:03.922884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions_train = []\nimage_features_train = []\nall_qs = {}\n\n#change format for better performing\nfor idx, imq in img_q.items():\n  for ques in imq:\n    all_qs[ques[0]] = {'question':ques[1], 'image_id': str(idx)}\nall_qs[131087000]","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:52:07.884628Z","iopub.execute_input":"2021-10-08T07:52:07.885273Z","iopub.status.idle":"2021-10-08T07:52:07.900441Z","shell.execute_reply.started":"2021-10-08T07:52:07.885222Z","shell.execute_reply":"2021-10-08T07:52:07.89923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx in q_train_idx:\n  questions_train.append(all_qs[idx]['question'])\n  image_features_train.append(img[all_qs[idx]['image_id']])","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:52:14.451922Z","iopub.execute_input":"2021-10-08T07:52:14.452253Z","iopub.status.idle":"2021-10-08T07:52:14.45871Z","shell.execute_reply.started":"2021-10-08T07:52:14.452222Z","shell.execute_reply":"2021-10-08T07:52:14.457827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions_val = []\nimage_features_val = []\n\nfor idx in q_val_idx:\n  questions_val.append(all_qs[idx]['question'])\n  image_features_val.append(img[all_qs[idx]['image_id']])","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:52:18.517333Z","iopub.execute_input":"2021-10-08T07:52:18.518072Z","iopub.status.idle":"2021-10-08T07:52:18.524846Z","shell.execute_reply.started":"2021-10-08T07:52:18.518034Z","shell.execute_reply":"2021-10-08T07:52:18.523861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"questions_test = []\nimage_features_test = []\n\nfor idx in q_test_idx:\n  questions_test.append(all_qs[idx]['question'])\n  image_features_test.append(img[all_qs[idx]['image_id']])","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:52:26.954609Z","iopub.execute_input":"2021-10-08T07:52:26.95598Z","iopub.status.idle":"2021-10-08T07:52:26.962771Z","shell.execute_reply.started":"2021-10-08T07:52:26.955907Z","shell.execute_reply":"2021-10-08T07:52:26.961794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\nbertmodel = transformers.BertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:53:46.520564Z","iopub.execute_input":"2021-10-08T07:53:46.520883Z","iopub.status.idle":"2021-10-08T07:54:06.572465Z","shell.execute_reply.started":"2021-10-08T07:53:46.520847Z","shell.execute_reply":"2021-10-08T07:54:06.571035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTClassification(torch.nn.Module):\n    def __init__ (self):\n        super(BERTClassification, self).__init__()\n        self.bert = bertmodel\n        \n    def forward(self, text):\n        hidden_state = self.bert(text)['last_hidden_state']\n        result = torch.mean(hidden_state, dim=1)\n        return result","metadata":{"execution":{"iopub.status.busy":"2021-10-08T07:53:37.733868Z","iopub.status.idle":"2021-10-08T07:53:37.734242Z","shell.execute_reply.started":"2021-10-08T07:53:37.73406Z","shell.execute_reply":"2021-10-08T07:53:37.734081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmodel = BERTClassification()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad_bert(b):\n    v = [tokenizer.encode(x) for x in b]\n    # compute max length of a sequence in this minibatch\n    l = max(map(len,v))\n    return torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in v])\n\npadded_train = pad_bert(questions_train)\npadded_val = pad_bert(questions_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_test = pad_bert(questions_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n  question_features_train = tmodel(padded_train)\nwith torch.no_grad():\n  question_features_val = tmodel(padded_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n  question_features_test = tmodel(padded_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = torch.utils.data.TensorDataset(question_features_train, torch.tensor(image_features_train), torch.tensor(label_train))\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataset = torch.utils.data.TensorDataset(question_features_val, torch.tensor(image_features_val), torch.tensor(label_val))\nval_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VQA2(nn.Module):\n    def __init__(self, features_size):\n        super(type(self), self).__init__()\n        #text feature\n        self.bert_out = torch.nn.Linear(768, 512)\n        self.linear = nn.Sequential(\n            nn.BatchNorm1d(features_size),\n            nn.Linear(features_size, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),         \n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Linear(128, 10),\n            nn.BatchNorm1d(10),\n        )\n                \n        \n    def forward(self, text, image):\n        text_f = self.bert_out(text)\n        feature = torch.cat([text_f,image], dim=1)\n        logits = self.linear(feature)\n        logits = nn.functional.softmax(logits, dim=1)\n        return logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vqa2 = VQA2(1024)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.08\nepochs = 10\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(vqa2.parameters(), lr=learning_rate)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (text, image, y) in enumerate(dataloader):        \n        # Compute prediction and loss\n        pred = model(text, image)\n        loss = loss_fn(pred, y)\n        \n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(text)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for text, image, y in dataloader:\n            pred = model(text, image)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n            \n    test_loss /= size\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, vqa2, loss_fn, optimizer)\n    test_loop(val_dataloader, vqa2, loss_fn)\nprint(\"Done!\")","metadata":{},"execution_count":null,"outputs":[]}]}